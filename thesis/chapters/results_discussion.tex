\chapter{Results and Discussion}\label{chap:results_discussion}
\section{Results}
\subsection{Binary Classification}
\subsubsection{Metrics}
When assessing the performance of a binary prediction model using a validation dataset containing known target values, four key metrics come into play:

\begin{itemize}
  \item True Positives (TP): The number of correctly predicted active cases.
  \item True Negatives (TN): The number of correctly predicted inactive cases.
  \item False Positives (FP): The number of incorrectly predicted active cases.
  \item False Negatives (FN): The number of incorrectly predicted inactive cases.
\end{itemize}

Using these four values, it is possible to construct a confusion matrix, as illustrated in Figure~\ref{tab:confusion_matrix}.


\begin{table}[h]
  \centering
  \caption{Confusion Matrix}
  \label{tab:confusion_matrix}
  \setlength{\tabcolsep}{10pt} % Adjust cell padding
  \renewcommand{\arraystretch}{1.5} % Adjust cell height
  \begin{tabular}{|c|c|c|}
  \cline{2-3}
  \multicolumn{1}{c|}{} & Actual Negative & Actual Positive \\
  \hline
  Predicted Negative & TN & FN \\
  \hline
  Predicted Positive & FP & TP \\
  \hline
  \end{tabular}
\end{table}


Several evaluation metrics can help assess the performance of a predictive model. These metrics include:

\begin{itemize}
  \item \textbf{Accuracy}: The ratio of correctly classified instances (TP) and (TN) to the total number of instances. It provides a general measure of the model's correctness.
  \[ \text{Accuracy}  A = \frac{TP + TN}{TP + TN + FP + FN} \]

  \item \textbf{Precision (P)}: The proportion of correctly predicted active cases (TP) to all instances predicted as active (TP + FP). 
  \[ \text{Precision } P = \frac{TP}{TP + FP} \]

  \item \textbf{Recall (R) or Sensitivity or True Positive Rate (TPR)}: The proportion of correctly predicted active cases (TP) to all actual active cases (TP + FN).
  \[ \text{Recall } R = \frac{TP}{TP + FN} \]

  \item \textbf{F1 Score}: The harmonic mean of precision and recall, which balances the trade-off between false positives and false negatives.
  \[ F1 = \frac{2 \cdot P \cdot R}{P + R} \]

  \item \textbf{True Negative Rate (TNR) Specificity}: The proportion of correctly predicted inactive cases (TN) to all actual inactive cases (TN + FP).
  \[ TNR = \frac{TN}{TN + FP} \]

  \item \textbf{Receiver Operating Characteristic (ROC) Curve}: A graphical representation of the model's performance across different classification thresholds. It plots the true positive rate (TNR) against the false positive rate (FPR).
  
\end{itemize}

The majority of assay endpoints exhibit an imbalance in the distribution of active (positive) and inactive (negative) compounds when employing binarized toxicity hitcalls. Mostly the negative class significantly outweighs the postive class. Imbalanced datasets can lead to skewed performance metrics, as the model may perform well on the majority class but poorly on the minority class. To address imbalanced datasets, additional metrics such as macro-averaged and weighted-averaged metrics can be taken into account.

In macro-averaging, the metric is computed separately for each class, and then an unweighted average is taken. This approach assigns equal importance to each class, regardless of their representation within the dataset. As an example, macro-averaged precision calculates the unweighted average of precision across all classes, whereas the weighted-average of precision considers the impact of class prevalence.
\[ \text{Macro-Precision } MP = \frac{1}{N} \sum_{i=1}^{N} P_i \] 
\[ \text{Weighted-Precision } WP = \frac{1}{N} \sum_{i=1}^{N} \left(\frac{TP_i}{TP_i + FP_i}\right) \cdot \frac{N_i}{N} \]

In both cases, $N$ is the total number of classes (e.g. $N=2$ for the binary case), and $N_i$ represents the number of samples in class $i$. Similarly the macro-averaged and weighted-averaged recall and F1 score can be calculated.

\subsubsection{Performance}
We evaluated the model performance using the previously mentioned metrics. The following figures depict the performance metrics for the default classification threshold. We used macro-averaged, weighted-averaged, and separately sliced metrics for the positive and negative classes, employing five different estimators. These scatter plots visually represent all target assay endpoint models applied to the internal validation dataset, utilizing the binarized hitcall without cytotoxicity correction as the target variable and an XGBoost classifier as the underlying feature selection model.

The marker size in the scatter plots corresponds to the number of compounds in the validation dataset. Boxplots represent the estimators and illustrate the distribution of performance metrics across the target assay endpoint models. The table, located adjacent to the scatter plot, provides the average metrics for the estimators across all target assay endpoint models.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{figures/hitcall_classification_xgb_val_default_macro_avg.png}
  \caption{}
~\label{fig:hitcall_classification_xgb_val_default_weighted_avg}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{figures/hitcall_classification_xgb_val_default_weighted_avg.png}
  \caption{}
~\label{fig:hitcall_classification_xgb_val_default_weighted_avg}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{figures/hitcall_classification_xgb_val_default_true.png}
  \caption{}
~\label{fig:hitcall_classification_xgb_val_default_true}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{figures/hitcall_classification_xgb_val_default_false.png}
  \caption{}
~\label{fig:hitcall_classification_xgb_val_default_false}
\end{figure}


This is only a slice of the results and ever combination of target variable, feature selection model, and assay endpoint is available in a dashbaord.
\begin{enumerate}
  \item target variables: hitcall, hitcall\_cytotox, hitcall\_cytotox\_corr
  \item feature selection models: XGBoost, Random Forest
  \item 370 assay endpoint
\end{enumerate}
