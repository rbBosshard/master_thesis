\chapter{Related work}\label{chap:related_work}

The recent advancements in machine learning-based prediction of toxicity endpoints, driven primarily by the efforts in drug discovery, were summarized in~\cite{cavasotto2022}. This study underscores that machine learning approaches demonstrate varying performance levels across diverse toxicity endpoints, with commonly studied ones including acute oral toxicity, hepatotoxicity, cardiotoxicity, and mutagenicity but also those endpoints from the popular Tox21 data challenge~\cite{richard2021}. The ability to predict toxicity depends significantly on the characteristics of the datasets, including differences in complexity, class distribution, and the chemical space they encompass, making it challenging to directly compare algorithm performance.

Similar to MLinvitroTox, MS2Tox~\cite{peets2022} represents another machine learning approach within the realm of predicting ecotoxicological hazards for unidentified compounds through nontarget HRMS/MS analysis. Both approaches adopt a common strategy of building their ML models based on molecular fingerprints derived from chemical structure, used to make predictions on environmental samples, utilizing fingerprints from fragmentation spectra calculated by SIRIUS+CSI:FingerID. However, ML2Tox diverges in terms of the toxicity data employed for training and testing, with its focus on toxicity data concerning \emph{in vivo} fish lethal concentrations from CompTox~\cite{williams2017}. This is in contrast to MLinvitroTox, which relies on \emph{in vitro} toxicity data from ToxCast/Tox21. Additionally, unlike MLinvitroTox, which exclusively relies on molecular fingerprints and does not utilize any physicochemical properties, MS2Tox incorporates the molecular mass of the compound as an additional feature.

In a systematic investigation using Tox21 data~\cite{wu2021}, the impact of various modeling approaches on predictive toxicology were explored, with a focus on model performance and explainability trade-offs. The study found that endpoints with higher predictability, characterized by lower data imbalance and larger datasets, performed well regardless of the modeling approach or molecular representation. For less predictable endpoints, simpler models like Linear Regression performed similarly to complex ones, thereby emphasizing the importance of balancing predictability and interpretability. Moreover this study suggests consensus modeling and multi-task learning to enhance predictability and model performance across endpoints. In this thesis, the goal was established to not to overlook simpler models due to their higher interpretability and comparable performance. As recommended, no further explorations were conducted regarding the various molecular representations, and instead, a fixed set of molecular fingerprints was employed as the initial input features, with feature selection being applied to reduce the number of relevant features. Furthermore, a consensus modeling approach was adopted, where the final predictions are obtained by averaging the predictions across assay endpoints sharing the same attributes, including mechanistic and biological target.


A recent study~\cite{kretschmer2023} explores the coverage of large-scale datasets used in machine learning for biomolecular structures, revealing their limitations in representing the full range of known structures. As the chemical space is vast, it is questionable whether the toxicity training data is an informative subset to the true distribution aimed to learn, directly challenging the fundamental assumption in machine learning. However it is impossible to cover all possible compounds but the study suggests that the coverage of the chemical space should be considered when evaluating the performance of machine learning models. In this thesis, the coverage of the chemical space was not considered, as the focus was on the performance of the models and their ability to generalize to unseen data. However, the coverage of the chemical space could be considered in future work, as it could provide additional insights into the performance of the models.


